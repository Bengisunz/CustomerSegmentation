# -*- coding: utf-8 -*-
"""BDA502_ML_PROJECT_OnizBengisu.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/Bengisunz/f6fd235dd972a8e4ed495a545adc47cd/orguntitled0.ipynb

#Instacart Market Customer Segmentation Analysis

##Bengisu Öniz
##MEF University
##June, 2018
##This project is made for BDA 502 Introduction to Machine Learning Course.

#A. Introduction

##About the Data

The data set l chose is Instachart Market that is offered for the recruiting competition. It is represented at Kaggle.com. It contains describing customers' orders over time.* 'The dataset is anonymized and contains a sample of over 3 million grocery orders from more than 200,000 Instacart users. For each user, we provide between 4 and 100 of their orders, with the sequence of products purchased in each order.  '* [1]

##File Descriptions

Each entity (customer, product, order, aisle, etc.) has an associated unique id. Most of the files and variable names should be self-explanatory.

**aisles.csv**

 aisle_id,aisle  
 1,prepared soups salads  
 2,specialty cheeses  
 3,energy granola bars  
 
 
**departments.csv**

 department_id,department  
 1,frozen  
 2,other  
 3,bakery  
 ...
 
**order_products__*.csv **

These files specify which products were purchased in each order. order_products__prior.csv contains previous order contents for all customers. 'reordered' indicates that the customer has a previous order that contains the product. Note that some orders will have no reordered items. You may predict an explicit 'None' value for orders with no reordered items. See the evaluation page for full details.

 order_id,product_id,add_to_cart_order,reordered
 1,49302,1,1  
 1,11109,2,1  
 1,10246,3,0  
 ... 

**orders.csv**

This file tells to which set (prior, train, test) an order belongs. You are predicting reordered items only for the test set orders. 'order_dow' is the day of week.

 order_id,user_id,eval_set,order_number,order_dow,order_hour_of_day,days_since_prior_order  
 2539329,1,prior,1,2,08,  
 2398795,1,prior,2,3,07,15.0  
 473747,1,prior,3,3,12,21.0  
 
 ...
**products.csv**


 product_id,product_name,aisle_id,department_id
 1,Chocolate Sandwich Cookies,61,19  
 2,All-Seasons Salt,104,13  
 3,Robust Golden Unsweetened Oolong Tea,94,7  
 ...

#B. Research Questions

*   l would like to learn how many customer segments the market has according to products they bought. (Product-based/ Situational segmentation)

*   How are these customer are segmented? Which kind of products are decisive for segments?

*   Which product categories are similar to each other?

*  Will the dimension reduction techniques be meaningful for the model?

#C. Exploratory Data Analysis
"""

!pip install pandas  #installing necessary libraries
!pip install numpy 
!pip install matplotlib
!pip install seaborn

import pandas as pd # importing necessary libraries
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import io

from google.colab import files  #uploading all files
uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))

aisles = pd.read_csv(io.StringIO(uploaded['aisles.csv'].decode('utf-8')))   #reading files
prior_df = pd.read_csv(io.StringIO(uploaded['order_products__prior1.csv'].decode('utf-8')))
orders_df = pd.read_csv(io.StringIO(uploaded['orders2.csv'].decode('utf-8')))
products_df = pd.read_csv(io.StringIO(uploaded['products.csv'].decode('utf-8')))
departments_df = pd.read_csv(io.StringIO(uploaded['departments.csv'].decode('utf-8')))

orders_df.head(2)

products_df.head(15)

aisles.head(5)

departments_df.head(5)

products_df = pd.merge(products_df, aisles, on='aisle_id', how='left')
products_df.head()

products_df = pd.merge(products_df, departments_df, on='department_id', how='left')
products_df.head()

order_products_prior_df = pd.merge(prior_df, orders_df, on='order_id', how='left')
order_products_prior_df.head(3)

order_products_prior_df = pd.merge(order_products_prior_df, products_df, on='product_id', how='left')  #merging files

print(order_products_prior_df.dtypes) #seeing the data types

print(order_products_prior_df.describe())

order_products_prior_df.isnull().sum().sort_values(ascending=False) #checking null values

"""There is no null value in the dataset"""

unique_orders = len(set(order_products_prior_df.order_id))
unique_products = len(set(order_products_prior_df.product_id))
unique_customers= len(set(order_products_prior_df.user_id))
unique_aisle= len(set(order_products_prior_df.aisle))

print("The number of unique orders is ", unique_orders)
print("The number of unique products is ", unique_products)
print("The number of unique customers is ", unique_customers)
print("The number of unique aisle is ", unique_aisle)

"""The number of unique orders is  50998

The number of unique products is  29436

The number of unique customers is  40750

The number of unique aisle is  134
"""

import matplotlib.pyplot as plt  #importing visualizastion packages
import seaborn as sns

cnt_srs = order_products_prior_df['aisle'].value_counts().head(20)
plt.figure(figsize=(12,8))
sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, palette="Set2")
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Aisle', fontsize=12)
plt.xticks(rotation='vertical')
plt.show()

"""The most bought aisle is fresh fruits. It is no suprising all the aisles in this graph contains basic needs of goods."""

cnt_srsp = order_products_prior_df['aisle'].value_counts().reset_index().head(20)
cnt_srsp.columns = ['aisle', 'frequency_count']
print(cnt_srsp)

"""These most frequently bought items can be removed for segmentation"""

cnt_srs = order_products_prior_df['product_name'].value_counts().head(20)
plt.figure(figsize=(12,8))
sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, palette="Set2")
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Products', fontsize=12)
plt.xticks(rotation='vertical')
plt.show()

"""The most bought good is banana. All of the goods in this chart are from fresh fruits and vegetables"""

cnt_srs_ = order_products_prior_df['order_dow'].value_counts()
plt.figure(figsize=(12,8))
sns.barplot(cnt_srs_.index, cnt_srs_.values, alpha=0.8, palette="Set2")
plt.ylabel('Count', fontsize=12)
plt.xlabel('Day of week', fontsize=12)
plt.xticks(rotation='vertical')
plt.title("Frequency of order by week day", fontsize=15)
plt.show()

"""l think the days of 0 and 1 are weekend days."""

cnt_srsd = order_products_prior_df['order_hour_of_day'].value_counts()
plt.figure(figsize=(12,8))
sns.barplot(cnt_srsd.index, cnt_srsd.values, alpha=0.8, palette="Set2")
plt.ylabel('Count', fontsize=12)
plt.xlabel('Hour of day', fontsize=12)
plt.xticks(rotation='vertical')
plt.title("Frequency of order by hour of day", fontsize=15)
plt.show()

"""Mid of the day is the most crowded time."""

orderm = order_products_prior_df.groupby('order_id').agg({'add_to_cart_order': [max, 'count']})
orderm.mode()

"""Max amount of products in the basket is 6."""

_mt = pd.merge(prior_df,products_df, on = ['product_id','product_id']) #merging the files for the model
_mt = pd.merge(_mt,orders_df,on=['order_id','order_id'])
mt = pd.merge(_mt,aisles,on=['aisle_id','aisle_id'])
mt.head(2)

order_products_prior_df.head(2)

type(mt.order_hour_of_day)
print(mt.order_hour_of_day.describe())

"""l was planning to add the model the day and time of the transaction occured. Therefore, l label the time and day. l think they are one of the main features for segments. Even l want to do situational segmentation, l want to add some behavioral features."""

hourlabel = []
daylabel=[]

for index, row in mt.iterrows():
    #hour
    hour = int(row["order_hour_of_day"])
    if hour <=5 or hour>=19:
        hourlabel.append(4)
    elif hour <=19 and hour>=15:
        hourlabel.append(3)
    elif hour <=15 and hour>=11:
        hourlabel.append(2)
    elif hour <= 11 and hour >= 5:
        hourlabel.append(1)
    else:
        hourlabel.append('None')
     #day
    day=int(row["order_dow"])
    if day==1 or day==0:
        daylabel.append(2)
    else:
        daylabel.append(1)

print(daylabel)
print(hourlabel)
mt['hourlabel'] = hourlabel
mt['day'] = daylabel

mt.head(5)

mt['wk'] = 0   #weekend
mt['wk'] = np.where((mt['day'] == 1),  1, 0)

mt['wd'] = 0   #weekdays
mt['wd'] = np.where((mt['day'] == 2),  1, 0)

mt['morning'] = 0
mt['morning'] = np.where((mt['hourlabel'] == 1),  1, 0)

mt['noon'] = 0
mt['noon'] = np.where((mt['hourlabel'] == 2),  1, 0)

mt['afternoon'] = 0
mt['afternoon'] = np.where((mt['hourlabel'] == 3),  1, 0)

mt['night'] = 0
mt['night'] = np.where((mt['hourlabel'] == 4),  1, 0)

mt.head(5)

#dropping unnecessary columns
mt_t= mt.drop(columns=['aisle_id','department_id','eval_set','order_number','order_dow','order_hour_of_day','days_since_prior_order','order_id','product_id','add_to_cart_order','reordered','product_name'])
mt_t.head(5)

"""l had to drop unnecessary columns in order to do sparse matrix."""

#mt_t.columns.values[0] = "a1"  #faced some difficulties
#mt_t.columns.values[1] = "a2"
#mt_t.columns.values[2] = "a3"

mt_t.head(2)

mt2=mt_t
mt2=mt2.drop(columns=['hourlabel','day'])
#mt2=mt2.drop(columns=['a1','a2','a3','hourlabel',	'day'])

mt3= mt2.drop(columns=['wk',	'wd',	'morning','noon',	'afternoon',	'night'])

"""Here l realize that l can not put time and day features to my sparse matrix. l need another matrixs for each labels then l merge all the matrix. Unfortunately, l cannot do this."""

mt3.head(2)

#@title Here is the sparse matrix
cust_prod = pd.crosstab(mt3['user_id'], mt3['aisle'])
cust_prod.head(5)

type(cust_prod)

cust_prod.keys()

"""#D. Data Analysis

###Implementing a Principal Component Analysis

l have 134 features/aisles. l want them to reduce by using Principal Component Analysis.
"""

from sklearn.decomposition import PCA 
pca=PCA()
pca.fit(cust_prod)
cumsum=np.cumsum(pca.explained_variance_ratio_)
d=np.argmax(cumsum>=0.80)+1

print("The number of variable for PCA", d)

"""l want pca tranformed data to explain %80 of all the features in the original data.
Therefore l use formule above. It says l need to have 30 components
"""

var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4))
plt.plot(var1)
plt.xlim([10,40])
plt.ylim([0.5,0.9])
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')
plt.show()

"""This is another way to have right number of components in PCA. It seems 25 is enough."""

pca = PCA(n_components=30)
pca.fit(cust_prod)
pca_samples = pca.transform(cust_prod)

ps = pd.DataFrame(pca_samples)
ps.head(15)

"""Now l have 30 features"""

from sklearn.cluster import KMeans   #importing libraries for the model
from sklearn.metrics import silhouette_samples, silhouette_score
import matplotlib.cm as cm

"""###Finding the right number of cluster"""

def RunSilhouetteForNClusters(X):
    range_n_clusters = [10, 20, 30, 40, 50]
    for n_clusters in range_n_clusters:
        print("n:" + str(n_clusters))
        # Create a subplot with 1 row and 2 columns
        fig, (ax1, ax2) = plt.subplots(1, 2)
        fig.set_size_inches(18, 7)

        # The 1st subplot is the silhouette plot
        # The silhouette coefficient can range from -1, 1 but in this example all
        # lie within [-0.1, 1]
        ax1.set_xlim([-0.1, 1])
        # The (n_clusters+1)*10 is for inserting blank space between silhouette
        # plots of individual clusters, to demarcate them clearly.
        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

        # Initialize the clusterer with n_clusters value and a random generator
        # seed of 10 for reproducibility.
        clusterer = KMeans(n_clusters=n_clusters, random_state=10)
        cluster_labels = clusterer.fit_predict(X)

        # The silhouette_score gives the average value for all the samples.
        # This gives a perspective into the density and separation of the formed
        # clusters
        silhouette_avg = silhouette_score(X, cluster_labels)
        print("For n_clusters =", n_clusters,
              "The average silhouette_score is :", silhouette_avg)

        # Compute the silhouette scores for each sample
        sample_silhouette_values = silhouette_samples(X, cluster_labels)

        y_lower = 10
        for i in range(n_clusters):
            # Aggregate the silhouette scores for samples belonging to
            # cluster i, and sort them
            ith_cluster_silhouette_values = \
                sample_silhouette_values[cluster_labels == i]

            ith_cluster_silhouette_values.sort()

            size_cluster_i = ith_cluster_silhouette_values.shape[0]
            y_upper = y_lower + size_cluster_i

            color = cm.Spectral(float(i) / n_clusters)
            print(color)
            ax1.fill_betweenx(np.arange(y_lower, y_upper),
                              0, ith_cluster_silhouette_values,
                              facecolor=color, edgecolor=color, alpha=0.7)

            # Label the silhouette plots with their cluster numbers at the middle
            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

            # Compute the new y_lower for next plot
            y_lower = y_upper + 10  # 10 for the 0 samples

        ax1.set_title("The silhouette plot for the various clusters.")
        ax1.set_xlabel("The silhouette coefficient values")
        ax1.set_ylabel("Cluster label")

        # The vertical line for average silhouette score of all the values
        ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

        ax1.set_yticks([])  # Clear the yaxis labels / ticks
        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

        # 2nd Plot showing the actual clusters formed
        colors = cm.Spectral(cluster_labels.astype(float) / n_clusters)
        ax2.scatter(X.iloc[:, 0], X.iloc[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                    c=colors, edgecolor='k')
        plt.show()


        # Labeling the clusters
        centers = clusterer.cluster_centers_
        # Draw white circles at cluster centers
        ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                    c="white", alpha=1, s=200, edgecolor='k')

        for i, c in enumerate(centers):
            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                        s=50, edgecolor='k')

        ax2.set_title("The visualization of the clustered data.")
        ax2.set_xlabel("Feature space for the 1st feature")
        ax2.set_ylabel("Feature space for the 2nd feature")

        plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                      "with n_clusters = %d" % n_clusters),
                     fontsize=14, fontweight='bold')

        plt.show()
        
#[2]        
RunSilhouetteForNClusters(ps)
print("finish")

"""l do k-means clustering. l want to find the right number k/cluster. Therefore, l check the silhoutte scores for different number of clusters"""

def RunSilhouetteForNClusters(X):
    range_n_clusters = [3, 5, 7, 10, 15]
    for n_clusters in range_n_clusters:
        print("n:" + str(n_clusters))
        # Create a subplot with 1 row and 2 columns
        fig, (ax1, ax2) = plt.subplots(1, 2)
        fig.set_size_inches(18, 7)

        # The 1st subplot is the silhouette plot
        # The silhouette coefficient can range from -1, 1 but in this example all
        # lie within [-0.1, 1]
        ax1.set_xlim([-0.1, 1])
        # The (n_clusters+1)*10 is for inserting blank space between silhouette
        # plots of individual clusters, to demarcate them clearly.
        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

        # Initialize the clusterer with n_clusters value and a random generator
        # seed of 10 for reproducibility.
        clusterer = KMeans(n_clusters=n_clusters, random_state=10)
        cluster_labels = clusterer.fit_predict(X)

        # The silhouette_score gives the average value for all the samples.
        # This gives a perspective into the density and separation of the formed
        # clusters
        silhouette_avg = silhouette_score(X, cluster_labels)
        print("For n_clusters =", n_clusters,
              "The average silhouette_score is :", silhouette_avg)

        # Compute the silhouette scores for each sample
        sample_silhouette_values = silhouette_samples(X, cluster_labels)

        y_lower = 10
        for i in range(n_clusters):
            # Aggregate the silhouette scores for samples belonging to
            # cluster i, and sort them
            ith_cluster_silhouette_values = \
                sample_silhouette_values[cluster_labels == i]

            ith_cluster_silhouette_values.sort()

            size_cluster_i = ith_cluster_silhouette_values.shape[0]
            y_upper = y_lower + size_cluster_i

            color = cm.Spectral(float(i) / n_clusters)
            print(color)
            ax1.fill_betweenx(np.arange(y_lower, y_upper),
                              0, ith_cluster_silhouette_values,
                              facecolor=color, edgecolor=color, alpha=0.7)

            # Label the silhouette plots with their cluster numbers at the middle
            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

            # Compute the new y_lower for next plot
            y_lower = y_upper + 10  # 10 for the 0 samples

        ax1.set_title("The silhouette plot for the various clusters.")
        ax1.set_xlabel("The silhouette coefficient values")
        ax1.set_ylabel("Cluster label")

        # The vertical line for average silhouette score of all the values
        ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

        ax1.set_yticks([])  # Clear the yaxis labels / ticks
        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

        # 2nd Plot showing the actual clusters formed
        colors = cm.Spectral(cluster_labels.astype(float) / n_clusters)
        ax2.scatter(X.iloc[:, 0], X.iloc[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                    c=colors, edgecolor='k')
        plt.show()


        # Labeling the clusters
        centers = clusterer.cluster_centers_
        # Draw white circles at cluster centers
        ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                    c="white", alpha=1, s=200, edgecolor='k')

        for i, c in enumerate(centers):
            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                        s=50, edgecolor='k')

        ax2.set_title("The visualization of the clustered data.")
        ax2.set_xlabel("Feature space for the 1st feature")
        ax2.set_ylabel("Feature space for the 2nd feature")

        plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                      "with n_clusters = %d" % n_clusters),
                     fontsize=14, fontweight='bold')

        plt.show()
        
#[2]        
RunSilhouetteForNClusters(ps)
print("finish")

"""For n_clusters = 3 The average silhouette_score is : 0.25022206981333006


For n_clusters = 5 The average silhouette_score is : 0.24814031205151454


For n_clusters = 10 The average silhouette_score is : 0.1451122368624305


For n_clusters = 15 The average silhouette_score is : 0.1307708412636545


For n_clusters = 20 The average silhouette_score is : 0.0794177512090574


For n_clusters = 30 The average silhouette_score is : 0.06786958075061109

For n_clusters = 40 The average silhouette_score is : 0.05958621260348865

For n_clusters = 50 The average silhouette_score is : 0.06277484215752366



**Plots of different silhouette scores**

[k:3](https://www.dropbox.com/s/j6s84ogmx7utobh/k3.png?dl=0) , [k:5](https://www.dropbox.com/s/cghez353vpryuj4/k5.png?dl=0) , [k:10](https://www.dropbox.com/s/lgu3cj650o6tkcx/k10.png?dl=0) , [k:15](https://www.dropbox.com/s/m0tio8v51kabk9t/k15.png?dl=0) ,  [k:20](https://www.dropbox.com/s/3b1rd9i1mpo2o07/k20.png?dl=0)

###Clustering with k=10
"""

from sklearn.cluster import KMeans  #model implementation


n_clusters=3
clusterer = KMeans(n_clusters=n_clusters,random_state=42).fit(ps)
centers = clusterer.cluster_centers_
cluster_labels = clusterer.fit_predict(ps)

print(centers)

final =cust_prod.copy()
final["cluster"] = cluster_labels
print(final.head(10))

print (final.shape)

f,arr = plt.subplots(2,2,sharex=True,figsize=(15,15))

c1_count = len(final[final['cluster']==0])

c0 = final[final['cluster']==0].drop('cluster',axis=1).mean()
arr[0,0].bar(range(len(final.drop('cluster',axis=1).columns)),c0)
c1 = final[final['cluster']==1].drop('cluster',axis=1).mean()
arr[0,1].bar(range(len(final.drop('cluster',axis=1).columns)),c1)
c2 = final[final['cluster']==2].drop('cluster',axis=1).mean()
arr[1,0].bar(range(len(final.drop('cluster',axis=1).columns)),c2)

plt.show()

"""I want to check top 10 goods that are bought from each clusters."""

c0.sort_values(ascending=False)[0:10]

"""Customers in this cluster may have baby"""

c1.sort_values(ascending=False)[0:10]

c2.sort_values(ascending=False)[0:10]

"""These are yogurt lovers."""

c3.sort_values(ascending=False)[0:10]

c4.sort_values(ascending=False)[0:10]

c5.sort_values(ascending=False)[0:10]

"""###Clustering with k=3 and without the most bought items

l want to delete the most bought items then make a cluster analysis again.
"""

secondcust =mt3.copy()
secondcust.head()


secondcust = secondcust[secondcust.aisle != 'fresh vegetables'] 
secondcust = secondcust[secondcust.aisle != 'chips pretzels'] 
secondcust = secondcust[secondcust.aisle != 'packaged vegetables fruits'] 
secondcust = secondcust[secondcust.aisle != 'yogurt'] 
secondcust = secondcust[secondcust.aisle != 'packaged cheese'] 
secondcust = secondcust[secondcust.aisle != 'milk'] 
secondcust = secondcust[secondcust.aisle != 'water seltzer sparkling water'] 
secondcust = secondcust[secondcust.aisle != 'bread'] 
secondcust = secondcust[secondcust.aisle != 'fresh fruits']

unique_aisle_= len(set(secondcust.aisle))
print(unique_aisle_)

secondcust = pd.crosstab(secondcust['user_id'], mt3['aisle'])

secondcust.head(5)


#deleting all the most common bought items 

pcat = PCA(n_components=30)
pcat.fit(secondcust)
pca_samplest = pcat.transform(secondcust)

pst = pd.DataFrame(pca_samplest)
pst.head(15)

n_clusters=3
clusterert = KMeans(n_clusters=n_clusters,random_state=42).fit(pst)
centerst = clusterert.cluster_centers_
clustert_labels = clusterert.fit_predict(pst)

finalt =secondcust.copy()
finalt["cluster"] = clustert_labels

secondcust = secondcust.copy()
secondcust['cluster'] = clustert_labels

c11_count = len(secondcust[secondcust['cluster']==0])

c10 = secondcust[secondcust['cluster']==0].drop('cluster',axis=1).mean()
arr[0,0].bar(range(len(secondcust.drop('cluster',axis=1).columns)),c10)
c11 = secondcust[secondcust['cluster']==1].drop('cluster',axis=1).mean()
arr[0,1].bar(range(len(secondcust.drop('cluster',axis=1).columns)),c11)
c12 = secondcust[secondcust['cluster']==2].drop('cluster',axis=1).mean()
arr[1,0].bar(range(len(secondcust.drop('cluster',axis=1).columns)),c12)


print(c10.sort_values(ascending=False)[0:10])
print("---------------------------------------")
print("---------------------------------------")

print(c11.sort_values(ascending=False)[0:10])
print("---------------------------------------")
print("---------------------------------------")

print(c12.sort_values(ascending=False)[0:10])

"""print(c10.sort_values(ascending=False)[0:10])

aisle_y

* energy granola bars    0.894325
* refrigerated           0.880320
* frozen meals           0.696981
* ice cream ice          0.634776
* crackers               0.591852
* frozen produce         0.563296
* cereal                 0.431066
* lunch meat             0.424700
* soup broth bouillon    0.367952
* eggs                   0.358494



print(c11.sort_values(ascending=False)[0:10])

aisle_y
* eggs                    0.158422
* frozen produce          0.153014
* ice cream ice           0.132451 
* refrigerated            0.129335
* fresh herbs             0.129121
* soft drinks             0.120963
* packaged produce        0.116533
* crackers                0.114577
* lunch meat              0.113416
* fresh dips tapenades    0.112622
dtype: float64


print(c12.sort_values(ascending=False)[0:10])

aisle_y
* baby food formula      6.620690
* frozen produce         0.406534
* crackers               0.399274
* refrigerated           0.337568
* cereal                 0.315789
* eggs                   0.312160
* frozen breakfast       0.308530
* lunch meat             0.290381
* energy granola bars    0.288566
* dry pasta              0.257713

###Clustering with t-SNE
"""

from sklearn.manifold import TSNE
X_embedded = TSNE(n_components=2).fit_transform(cust_prod)
print(X_embedded.shape)
print("done")

from sklearn.cluster import KMeans

n_clusters=3
clusterert = KMeans(n_clusters=n_clusters,random_state=42).fit(X_embedded)
centerst = clusterert.cluster_centers_
clustert_labels = clusterert.fit_predict(X_embedded)

print(centerst)

finaltt =cust_prod.copy()
finaltt["cluster"] = clustert_labels

print (finaltt.shape)

thirdcust = cust_prod.copy()
thirdcust['cluster'] = clustert_labels

thirdcust.head(1)

C20_count = len(thirdcust[thirdcust['cluster']==0])

c20 = thirdcust[thirdcust['cluster']==0].drop('cluster',axis=1).mean()
c21 = thirdcust[thirdcust['cluster']==1].drop('cluster',axis=1).mean()
c22 = thirdcust[thirdcust['cluster']==2].drop('cluster',axis=1).mean()

print(c20.sort_values(ascending=False)[0:10])
print("-----------------------------------------")
print("-----------------------------------------")

print(c21.sort_values(ascending=False)[0:10])
print("-----------------------------------------")
print("-----------------------------------------")
print(c22.sort_values(ascending=False)[0:10])

"""**cluster results- t-SNE with total order data**

aisle_y
* yogurt                           0.925243
* fresh fruits                     0.646664
* water seltzer sparkling water    0.559923
* chips pretzels                   0.381747
* refrigerated                     0.335401
* energy granola bars              0.328612
* ice cream ice                    0.325511
* milk                             0.322662
* packaged vegetables fruits       0.320734
* frozen meals                     0.307157

aisle_y
* fresh fruits                  0.485821
* packaged vegetables fruits    0.321768
* fresh vegetables              0.313724
* packaged cheese               0.254083
* packaged produce              0.253758
* milk                          0.197449
* bread                         0.147315
* frozen produce                0.145771
* soy lactosefree               0.139027
* chips pretzels                0.134151


aisle_y
* fresh vegetables                 2.817152
* fresh fruits                     2.675186
* packaged vegetables fruits       1.231603
* yogurt                           0.683302
* packaged cheese                  0.542911
* milk                             0.474229
* soy lactosefree                  0.358670
* baby food formula                0.334747
* bread                            0.327055
* water seltzer sparkling water    0.315789

###Clustering without dimension reduction
"""

cust_prod

n_clusters=3
clusterer4 = KMeans(n_clusters=n_clusters,random_state=42).fit(cust_prod)
center4 = clusterer4.cluster_centers_
cluster_labels4 = clusterer4.fit_predict(cust_prod)


final4 =cust_prod.copy()
final4["cluster"] = cluster_labels4


c41_count = len(secondcust[secondcust['cluster']==0])

c40 = final4[final4['cluster']==0].drop('cluster',axis=1).mean()
c41 = final4[final4['cluster']==1].drop('cluster',axis=1).mean()
c42 = final4[final4['cluster']==2].drop('cluster',axis=1).mean()

print(c40.sort_values(ascending=False)[0:10])
print("---------------------------------------")
print("---------------------------------------")
print(c41.sort_values(ascending=False)[0:10])
print("---------------------------------------")
print("---------------------------------------")
print(c42.sort_values(ascending=False)[0:10])

"""#Discussion & Conclusion

I did situational segmentations. Therefore, l can separate segments by looking at the goods they bought. l can name the clusters by looking at their most bought goods.

I would like to add the hour and the day features then have the different matrix. In the end, l can combine them.

I was surprised that k:3 has the highest silhouette score. l have guessed there would be around 6 clusters according to categories and the number of customers.

l used dimension reduction techniques before clustering. I implemented PCA and TSNE. In addition to them, l did clustering analysis without dimension reduction techniques. In the end, l had a different kind of clusters.

While doing this project, l have learned several lessons. First of all, code organization is so important if l want to run and share the codes on different platforms. It was not easy to prepare the dataset for the model. I should foresee the data structure for the model before l start to write the codes. I did some extra works but l cannot use them because l could not foresee the next steps so well. The worsts thing happened while doing this project was a memory problem. Because of this, l had to run the codes for several times.

#References

[1] Kaggle.com. (2018). Customer Segments with PCA | Kaggle. [online] Available at: https://www.kaggle.com/asindico/customer-segments-with-pca [Accessed 20 May. 2018].



[2] Scikit-learn.org. (2018). Selecting the number of clusters with silhouette analysis on KMeans clustering — scikit-learn 0.19.1 documentation. [online] Available at: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py m [Accessed May 30. 2018].
"""